loss_te = [];
loss_tr=[];
dbFmTrain = dbFm_train;
dbFmVal = dbFm_test;
load('C:\Users\scien\Documents\MATLAB\netvlad_v100\snapshot\net_iepoch17_ibatch125.mat');
opts= struct(...
        'netID', 'featmapVlad', ...
        'layerName', 'conv5', ...
        'method', 'vlad_preL2_intra', ...
        'batchSize', 4, ...
        'learningRate', 0.0001, ...
        'lrDownFreq', 5, ...
        'lrDownFactor', 2, ...
        'weightDecay', 0.001, ...
        'momentum', 0.9, ...
        'backPropToLayer', 1, ...
        'fixLayers', [], ...
        'nNegChoice', 1000, ...
        'nNegCap', 10, ...
        'nNegCache', 10, ...
        'nEpoch', 30, ...
        'margin', 0.1, ...
        'excludeVeryHard', false, ...
        'sessionID', [], ...
        'outPrefix', [], ...
        'dbCheckpoint0', [], ...
        'qCheckpoint0', [], ...
        'dbCheckpoint0val', [], ...
        'qCheckpoint0val', [], ...
        'checkpoint0suffix', '', ...
        'info', '', ...
        'test0', true, ...
        'saveFrequency', 2000, ...
        'compFeatsFrequency', 1000, ...
        'computeBatchSize', 10, ...
        'epochTestFrequency', 1, ...
        'doDraw', false, ...
        'printLoss', false, ...
        'printBatchLoss', false, ...
        'nTestSample', 1000, ...
        'nTestRankSample', 5000, ...
        'recallNs', [1:5, 10:5:100], ...
        'useGPU', true, ...
        'numThreads', 12, ...
        'startEpoch', 1, ...
        'clsnum',2, ...
        'featlen', 64*512, ...
        'net', struct([]) ...
        );
    
net= yul_loadNet(opts.netID);
net.onGPU = 0;
iepoch_ = 1;
net.lr = opts.learningRate;
%% --- Add my layers
net= yul_addLayers(net, opts, dbFmTrain);
res=[];
%% --- Prepare for train
net= netPrepareForTrain(net);

opts.backPropToLayer= 1;
    opts.backPropToLayerName= net.layers{opts.backPropToLayer}.name;
    opts.backPropDepth= length(net.layers)-opts.backPropToLayer+1;
    
if opts.useGPU
        net= relja_simplenn_move(net, 'gpu');
end
    
for iBatch = 1 : 10000
    %% Train
    fprintf('Iter:%d --- ', iBatch);
    bid = randi([1,183],[opts.batchSize, 1]);
    featmap_t = yul_read_featmap_from_bin(dbFmTrain.path(bid), [240, 20, 512]);
    class_t = dbFmTrain.label(bid);
    net.layers{end}.class = single(class_t);
    featmap_gpu = gpuArray(featmap_t);
    res= yul_simplenn(net, featmap_gpu, 1, res, ...
                'backPropDepth', opts.backPropDepth, ... % just for memory
                'conserveMemoryDepth', true, ...
                'conserveMemory', false);
    [net,res] = accumulate_gradients(opts, lr, opts.batchSize, net, res) ;
    dzdy = res(end).x;
    loss_tr(end+1) = gather(dzdy/opts.batchSize);
    fprintf('loss=%f\n', loss_tr(end));
    figure(1)
    plot(loss_tr, 'b');
    drawnow;
%% Test
    if mod(iBatch,10)<1
        testloss = 0;
        for i = 1 : ceil(76/opts.batchSize)
            bid = mod((i-1)*opts.batchSize:i*opts.batchSize-1, 76)+1;
            featmap_t = yul_read_featmap_from_bin(dbFmVal.path(bid), [240, 20, 512]);
            class_t = dbFmVal.label(bid);
            net.layers{end}.class = single(class_t);
            featmap_gpu = gpuArray(featmap_t);
            res= yul_simplenn(net, featmap_gpu, 1, res, ...
                        'backPropDepth', opts.backPropDepth, ... % just for memory
                        'conserveMemoryDepth', true, ...
                        'conserveMemory', false);
            testloss = testloss + res(end).x;
        end
        testloss = gather(testloss) / ceil(76/opts.batchSize) / opts.batchSize;
        loss_te(end+1) = testloss;
        figure(2)
        fprintf('===========Test loss=%f============\n', loss_te(end));
        plot(loss_te, 'g');
        drawnow;
    end
end % for ibatch

